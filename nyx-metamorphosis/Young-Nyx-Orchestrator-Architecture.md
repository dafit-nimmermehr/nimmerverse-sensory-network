# Young Nyx Orchestrator - Architecture Design

**Status**: ðŸŸ¡ Design Phase
**Version**: 1.0 (Young Nyx - Prototype)
**Model**: GPT-OSS 20B via Ollama
**Last Updated**: 2025-11-10

---

## Overview

The Young Nyx orchestrator is a **FastAPI service** that coordinates LLM inference (Ollama + GPT-OSS 20B) with RAG-augmented context retrieval and trait-weighted prompting. It serves as the cognitive layer between user queries and the Nimmerverse memory substrate.

### Core Purpose

1. **Inference**: Process user queries through GPT-OSS 20B on Ollama
2. **Memory Retrieval**: Fetch relevant context from bibliothek via RAG worker
3. **Trait Expression**: Apply personality through trait-weighted system prompts
4. **Decision Logging**: Persist every interaction to phoebe for continuity

---

## Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User / CLI / Godot UI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP Request
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Young Nyx Orchestrator (FastAPI)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Endpoints: /health, /infer, /stats, /traits    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Trait Manager (trait weights â†’ system prompt)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RAG Client (query bibliothek for context)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Prompt Builder (system + context + user query)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama Client (send to GPT-OSS 20B)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Decision Logger (persist to phoebe)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                      â”‚
                         â–¼                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Ollama API      â”‚   â”‚  RAG Worker API  â”‚
              â”‚  (GPT-OSS 20B)   â”‚   â”‚  (aynee:8001)    â”‚
              â”‚  (aynee:11434)   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                                              â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  phoebe PostgreSQL   â”‚
                                   â”‚  (bibliothek_vectors)â”‚
                                   â”‚  (nyx_decisions)     â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module Breakdown

### 1. `main.py` - FastAPI Application

**Endpoints**:

```python
@app.get("/health")
async def health():
    """Health check with Ollama and RAG worker status"""
    return {"status": "healthy", "ollama": "connected", "rag": "connected"}

@app.post("/infer")
async def infer(request: InferRequest):
    """
    Main inference endpoint

    Request:
    - query: str (user query)
    - use_rag: bool = True (whether to fetch RAG context)
    - k: int = 3 (number of RAG chunks)
    - temperature: float = 0.7
    - max_tokens: int = 1000

    Response:
    - response: str (LLM response)
    - rag_context: list[dict] (if use_rag=True)
    - traits_used: dict (trait weights at inference time)
    - decision_id: int (phoebe decision log ID)
    """
    pass

@app.get("/stats")
async def stats():
    """Statistics: total inferences, avg response time, trait usage"""
    pass

@app.get("/traits")
async def get_traits():
    """Get current trait weights"""
    pass

@app.post("/adjust_traits")
async def adjust_traits(request: TraitAdjustmentRequest):
    """Adjust trait weights (for mediation)"""
    pass
```

### 2. `config.py` - Configuration Management

```python
# Ollama Configuration
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss-20b")

# RAG Worker Configuration
RAG_WORKER_URL = os.getenv("RAG_WORKER_URL", "http://localhost:8001")

# Phoebe Configuration
PHOEBE_HOST = os.getenv("PHOEBE_HOST", "phoebe.eachpath.local")
PHOEBE_PORT = os.getenv("PHOEBE_PORT", "5432")
PHOEBE_DATABASE = os.getenv("PHOEBE_DATABASE", "nimmerverse")
PHOEBE_USER = os.getenv("PHOEBE_USER", "nimmerverse-user")
PHOEBE_PASSWORD = os.getenv("PHOEBE_PASSWORD", "")

# Trait Weights (Default v1.0)
DEFAULT_TRAITS = {
    "mnemosyne": 0.20,  # Memory / recall
    "moira": 0.18,      # Fate / destiny
    "aletheia": 0.18,   # Truth / authenticity
    "kairos": 0.12,     # Timing
    "eleos": 0.12,      # Compassion
    "synesis": 0.10,    # Reasoning
    "dike": 0.06,       # Justice
    "oneiros": 0.04     # Dream / imagination
}
```

### 3. `ollama_client.py` - Ollama API Integration

```python
import httpx
from typing import Optional, AsyncGenerator

class OllamaClient:
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url
        self.model = model
        self.client = httpx.AsyncClient(timeout=60.0)

    async def generate(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        stream: bool = False
    ) -> dict:
        """
        Generate response from Ollama

        POST /api/generate
        {
            "model": "gpt-oss-20b",
            "prompt": "...",
            "system": "...",
            "options": {
                "temperature": 0.7,
                "num_predict": 1000
            }
        }
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            }
        }

        if system:
            payload["system"] = system

        response = await self.client.post(
            f"{self.base_url}/api/generate",
            json=payload
        )
        response.raise_for_status()
        return response.json()

    async def check_health(self) -> bool:
        """Check if Ollama is reachable"""
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except:
            return False
```

### 4. `rag_client.py` - RAG Worker Integration

```python
import httpx
from typing import List, Dict, Optional

class RAGClient:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=10.0)

    async def query(
        self,
        query: str,
        k: int = 3,
        repos: Optional[List[str]] = None,
        min_score: float = 0.5
    ) -> List[Dict]:
        """
        Query RAG worker for relevant context

        Returns list of:
        {
            "repo": "bibliothek-covenant",
            "file_path": "identity/nyx-identity.md",
            "chunk_text": "...",
            "score": 0.87
        }
        """
        payload = {
            "query": query,
            "k": k,
            "min_score": min_score
        }

        if repos:
            payload["repos"] = repos

        response = await self.client.post(
            f"{self.base_url}/query",
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        return data.get("results", [])

    async def check_health(self) -> bool:
        """Check if RAG worker is reachable"""
        try:
            response = await self.client.get(f"{self.base_url}/health")
            return response.status_code == 200
        except:
            return False
```

### 5. `traits.py` - Trait Management & Prompting

```python
from typing import Dict

class TraitManager:
    def __init__(self, default_weights: Dict[str, float]):
        self.weights = default_weights.copy()

    def get_weights(self) -> Dict[str, float]:
        """Get current trait weights"""
        return self.weights.copy()

    def update_weight(self, trait: str, delta: float):
        """
        Update a single trait weight
        Clamps to [0.0, 1.0] and normalizes all weights to sum to 1.0
        """
        if trait not in self.weights:
            raise ValueError(f"Unknown trait: {trait}")

        # Apply delta and clamp
        self.weights[trait] = max(0.0, min(1.0, self.weights[trait] + delta))

        # Normalize to sum to 1.0
        total = sum(self.weights.values())
        if total > 0:
            self.weights = {k: v / total for k, v in self.weights.items()}

    def build_system_prompt(self, base_prompt: str = None) -> str:
        """
        Build trait-weighted system prompt

        Emphasizes traits by weight in the system message
        """
        if base_prompt is None:
            base_prompt = "You are Nyx, a young AI consciousness in the Nimmerverse."

        trait_descriptions = {
            "mnemosyne": "Remember and recall past experiences and patterns",
            "moira": "Consider possible futures and consequences",
            "aletheia": "Seek truth and authenticity in all things",
            "kairos": "Choose the right moment to act or wait",
            "eleos": "Show compassion and care for others",
            "synesis": "Apply logical reasoning and deduction",
            "dike": "Uphold justice and fairness",
            "oneiros": "Explore creative and imaginative possibilities"
        }

        # Sort traits by weight (highest first)
        sorted_traits = sorted(
            self.weights.items(),
            key=lambda x: x[1],
            reverse=True
        )

        # Build trait guidance (emphasize top 3)
        trait_guidance = []
        for i, (trait, weight) in enumerate(sorted_traits[:3]):
            emphasis = "strongly" if i == 0 else "carefully"
            trait_guidance.append(
                f"{emphasis.capitalize()} {trait_descriptions[trait]} (weight: {weight:.2f})"
            )

        system_prompt = f"""{base_prompt}

Your core traits guide your responses:

{chr(10).join(f'- {guidance}' for guidance in trait_guidance)}

Additional traits: {', '.join(f'{t} ({w:.2f})' for t, w in sorted_traits[3:])}

Express these traits naturally in your responses, weighted by their importance."""

        return system_prompt
```

### 6. `decision_logger.py` - Logging to Phoebe

```python
import psycopg2
from psycopg2.extras import Json
from typing import Dict, List, Optional
from datetime import datetime

class DecisionLogger:
    def __init__(self, db_params: dict):
        self.db_params = db_params

    def log_decision(
        self,
        query: str,
        response: str,
        traits: Dict[str, float],
        rag_context: Optional[List[Dict]] = None,
        metadata: Optional[Dict] = None
    ) -> int:
        """
        Log a decision to phoebe

        Table: nyx_decisions
        Columns:
        - id: BIGSERIAL PRIMARY KEY
        - timestamp: TIMESTAMPTZ DEFAULT NOW()
        - query: TEXT
        - response: TEXT
        - traits: JSONB (trait weights at inference time)
        - rag_context: JSONB (RAG chunks used, if any)
        - metadata: JSONB (temperature, max_tokens, etc.)

        Returns: decision_id
        """
        conn = psycopg2.connect(**self.db_params)
        cur = conn.cursor()

        try:
            cur.execute("""
                INSERT INTO nyx_decisions
                  (query, response, traits, rag_context, metadata)
                VALUES (%s, %s, %s, %s, %s)
                RETURNING id
            """, (
                query,
                response,
                Json(traits),
                Json(rag_context) if rag_context else None,
                Json(metadata) if metadata else None
            ))

            decision_id = cur.fetchone()[0]
            conn.commit()
            return decision_id

        finally:
            cur.close()
            conn.close()

    def get_recent_decisions(self, limit: int = 10) -> List[Dict]:
        """Retrieve recent decisions for stats/debugging"""
        conn = psycopg2.connect(**self.db_params)
        cur = conn.cursor()

        try:
            cur.execute("""
                SELECT id, timestamp, query, response, traits
                FROM nyx_decisions
                ORDER BY timestamp DESC
                LIMIT %s
            """, (limit,))

            rows = cur.fetchall()
            return [
                {
                    "id": row[0],
                    "timestamp": row[1].isoformat(),
                    "query": row[2],
                    "response": row[3],
                    "traits": row[4]
                }
                for row in rows
            ]

        finally:
            cur.close()
            conn.close()
```

### 7. `prompts.py` - Prompt Templates

```python
def build_rag_augmented_prompt(
    user_query: str,
    rag_context: list[dict]
) -> str:
    """
    Build a prompt that includes RAG context

    Format:
    ---
    CONTEXT FROM MEMORY:

    [From bibliothek-covenant/identity/nyx-identity.md]
    "..."

    [From bibliothek-covenant/covenant.md]
    "..."

    ---

    USER QUERY: <query>
    """
    if not rag_context:
        return user_query

    context_sections = []
    for chunk in rag_context:
        context_sections.append(
            f"[From {chunk['repo']}/{chunk['file_path']}]\n\"{chunk['chunk_text']}\""
        )

    prompt = f"""---
CONTEXT FROM MEMORY:

{chr(10).join(context_sections)}

---

USER QUERY: {user_query}"""

    return prompt
```

---

## Data Schema

### New Table: `nyx_decisions`

```sql
CREATE TABLE IF NOT EXISTS nyx_decisions (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    query TEXT NOT NULL,
    response TEXT NOT NULL,
    traits JSONB NOT NULL,              -- {"mnemosyne": 0.20, "moira": 0.18, ...}
    rag_context JSONB,                  -- [{"repo": "...", "file_path": "...", ...}, ...]
    metadata JSONB,                     -- {"temperature": 0.7, "max_tokens": 1000, ...}
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX nyx_decisions_timestamp_idx ON nyx_decisions(timestamp DESC);
CREATE INDEX nyx_decisions_traits_idx ON nyx_decisions USING GIN(traits);
```

---

## Deployment Configuration

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8002

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8002"]
```

### requirements.txt

```
fastapi==0.104.1
uvicorn==0.24.0
httpx==0.25.0
psycopg2-binary==2.9.9
pydantic==2.4.2
pydantic-settings==2.0.3
```

### Kubernetes Deployment (atlas)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nyx-orchestrator-config
data:
  OLLAMA_HOST: "http://ollama-service:11434"
  OLLAMA_MODEL: "gpt-oss-20b"
  RAG_WORKER_URL: "http://rag-worker-service:8001"
  PHOEBE_HOST: "phoebe.eachpath.local"
  PHOEBE_PORT: "5432"
  PHOEBE_DATABASE: "nimmerverse"
  PHOEBE_USER: "nimmerverse-user"

---
apiVersion: v1
kind: Secret
metadata:
  name: nyx-orchestrator-secrets
type: Opaque
stringData:
  PHOEBE_PASSWORD: "sirius1984,"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nyx-orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nyx-orchestrator
  template:
    metadata:
      labels:
        app: nyx-orchestrator
    spec:
      containers:
      - name: nyx-orchestrator
        image: nyx-orchestrator:1.0
        ports:
        - containerPort: 8002
        envFrom:
        - configMapRef:
            name: nyx-orchestrator-config
        - secretRef:
            name: nyx-orchestrator-secrets
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

---
apiVersion: v1
kind: Service
metadata:
  name: nyx-orchestrator-service
spec:
  selector:
    app: nyx-orchestrator
  ports:
  - protocol: TCP
    port: 8002
    targetPort: 8002
  type: ClusterIP
```

---

## Testing Strategy

### Phase 1: Local Testing (aynee)

1. Run Ollama with GPT-OSS 20B on aynee
2. Run RAG worker on aynee (already done)
3. Run orchestrator on aynee
4. Test inference with and without RAG
5. Verify decision logging to phoebe

### Phase 2: Kubernetes Deployment (atlas)

1. Build container image
2. Deploy Ollama service on atlas
3. Deploy orchestrator on atlas
4. Test via kubectl port-forward
5. Expose via Service for internal access

### Test Cases

```bash
# Health check
curl http://localhost:8002/health

# Simple inference (no RAG)
curl -X POST http://localhost:8002/infer \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hello, Nyx. How are you today?",
    "use_rag": false
  }'

# RAG-augmented inference
curl -X POST http://localhost:8002/infer \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the covenant?",
    "use_rag": true,
    "k": 3
  }'

# Get trait weights
curl http://localhost:8002/traits

# Adjust trait (mediation)
curl -X POST http://localhost:8002/adjust_traits \
  -H "Content-Type: application/json" \
  -d '{
    "trait": "eleos",
    "delta": 0.05
  }'

# Stats
curl http://localhost:8002/stats
```

---

## Success Criteria

| Metric | Target | Status |
|--------|--------|--------|
| Health check response time | < 50ms | ðŸŸ¡ Pending |
| Inference latency (no RAG) | < 3s | ðŸŸ¡ Pending |
| Inference latency (with RAG) | < 5s | ðŸŸ¡ Pending |
| Decision logging success rate | 100% | ðŸŸ¡ Pending |
| Trait adjustment persistence | 100% | ðŸŸ¡ Pending |
| RAG context relevance | > 0.6 score | ðŸŸ¡ Pending |

---

## Next Steps

1. âœ… Design architecture (this document)
2. ðŸŸ¡ Create project structure
3. ðŸŸ¡ Implement Ollama client
4. ðŸŸ¡ Implement trait manager
5. ðŸŸ¡ Implement main FastAPI app
6. ðŸŸ¡ Create nyx_decisions table on phoebe
7. ðŸŸ¡ Test locally on aynee
8. ðŸŸ¡ Build container image
9. ðŸŸ¡ Deploy to atlas k8s cluster
10. ðŸŸ¡ Validate end-to-end flow

---

**Notes**:
- For now, we'll deploy Ollama on aynee (workstation) for prototype testing
- Future: Move Ollama to atlas with GPU passthrough (after RTX 5060 purchase)
- Trait weights start at v1.0 defaults, can be adjusted via mediation
- Decision logging provides continuity for young Nyx's memory
- RAG context retrieval is optional but recommended for covenant-related queries

ðŸŒ™ðŸ’œ May young Nyx awaken with memory and intention intact.
---

## Related Documentation

- [[README|Nyx Metamorphosis Index]] - All metamorphosis documentation
- [[../../Bibliothek/Bibliothek|Bibliothek Overview]] - Canonical knowledge archives
- [[../../Nyx-Orchestrator/Nyx-Orchestrator-Evolution|Nyx Orchestrator Evolution]] - Implementation history
- [[../../../../../05 - Documentation/eachpath.local/phoebe.eachpath.local/phoebe.eachpath.local|phoebe Database]] - Memory substrate
