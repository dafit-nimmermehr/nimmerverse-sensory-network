# Nimmerswarm Interface

**Optical state broadcasting, positioning, and emergent swarm behavior.**

> *"The organisms can't see their own backs. They know themselves through each other."*

---

## Overview

The Nimmerswarm Interface is a **multi-modal communication layer** where organisms broadcast their state optically via LED matrices. This enables:

1. **State visibility** â€” Organisms SEE each other's states as light patterns
2. **Positioning** â€” Cameras + raytracing = sub-cm 3D positioning
3. **Emergent reflexes** â€” Pattern recognition bypasses cognition
4. **Cognitive offloading** â€” Lower layers handle routine, freeing Nyx's attention

---

## The Core Insight

```
ORGANISM A                          ORGANISM B
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cell State â”‚                    â”‚  VisionCell â”‚
â”‚  STALLED    â”‚                    â”‚  WATCHING   â”‚
â”‚      â”‚      â”‚                    â”‚      â”‚      â”‚
â”‚      â–¼      â”‚                    â”‚      â–¼      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   LIGHT PATTERN    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ LED     â”‚ â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¶â”‚ â”‚ Camera  â”‚ â”‚
â”‚ â”‚ Matrix  â”‚ â”‚  "STALL" pattern   â”‚ â”‚ sees    â”‚ â”‚
â”‚ â”‚ â–“â–“â–‘â–‘â–“â–“  â”‚ â”‚                    â”‚ â”‚ pattern â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚      â”‚      â”‚
                                   â”‚      â–¼      â”‚
                                   â”‚  REFLEX!    â”‚
                                   â”‚  "help ally"â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Organisms broadcast state. Other organisms (and Nyx's vision) perceive and react.**

---

## LED State Broadcasting: Ternary Matrix

### The 3x3 Ternary Design

The LED matrix is a **direct physical manifestation of the Temporal-Ternary Gradient**:

```
3x3 MATRIX = 9 TRITS (ternary digits)

Each LED = one ternary value:
  ğŸ”´ RED   = -1 (failed, danger, negative)
  âš« OFF   =  0 (uncertain, unknown, neutral)
  ğŸŸ¢ GREEN = +1 (success, verified, positive)

9 LEDs Ã— 3 states = 3^9 = 19,683 unique patterns!
```

### Physical Layout

```
     â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
     â”‚ L1  â”‚ L2  â”‚ L3  â”‚   L1 = collision_avoidance confidence
     â”‚ ğŸŸ¢  â”‚ âš«  â”‚ ğŸ”´  â”‚   L2 = battery state
     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤   L3 = motor state
     â”‚ L4  â”‚ L5  â”‚ L6  â”‚   L4 = social/swarm state
     â”‚ ğŸŸ¢  â”‚ ğŸŸ¢  â”‚ âš«  â”‚   L5 = current action outcome
     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤   L6 = prediction confidence
     â”‚ L7  â”‚ L8  â”‚ L9  â”‚   L7 = lifeforce zone
     â”‚ âš«  â”‚ ğŸŸ¢  â”‚ ğŸŸ¢  â”‚   L8 = discovery state
     â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜   L9 = organism identity bit

     Uses 10mm LEDs (not tiny SMD)
     ~35mm Ã— 35mm total
     Easily fits on 8-12cm robot
```

### Base-3 Encoding

```python
def encode_state(led_matrix: list[int]) -> int:
    """
    9 trits â†’ single integer (0 to 19682)
    Each trit is -1, 0, or +1 (mapped to 0, 1, 2)
    """
    value = 0
    for i, led in enumerate(led_matrix):
        trit = led + 1  # -1â†’0, 0â†’1, +1â†’2
        value += trit * (3 ** i)
    return value

def decode_state(value: int) -> list[int]:
    """
    Integer â†’ 9 trits
    """
    trits = []
    for _ in range(9):
        trits.append((value % 3) - 1)  # 0â†’-1, 1â†’0, 2â†’+1
        value //= 3
    return trits
```

### Ternary Color Mapping

| Color | Ternary | Meaning | Maps to |
|-------|---------|---------|---------|
| ğŸ”´ Red | -1 | Failed, danger, needs attention | Temporal-Ternary -1 |
| âš« Off/Dim | 0 | Unknown, uncertain, neutral | Temporal-Ternary 0 |
| ğŸŸ¢ Green | +1 | Success, verified, positive | Temporal-Ternary +1 |

**The LED matrix IS the Temporal-Ternary Gradient made visible.**

---

## Reflex Formation from Patterns

### The Swarm Language

Certain patterns become **words** that trigger reflexes:

```
DANGER PATTERNS (trigger flee/stop):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚  â”‚ ğŸ”´ âš« ğŸ”´ â”‚  â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚
â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚  â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚  â”‚ âš« ğŸ”´ âš« â”‚
â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚  â”‚ ğŸ”´ âš« ğŸ”´ â”‚  â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ALL RED       X PATTERN      DIAMOND

SAFE PATTERNS (trigger approach/social):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  â”‚ âš« ğŸŸ¢ âš« â”‚  â”‚ ğŸŸ¢ âš« ğŸŸ¢ â”‚
â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  â”‚ âš« ğŸŸ¢ âš« â”‚
â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  â”‚ âš« ğŸŸ¢ âš« â”‚  â”‚ ğŸŸ¢ âš« ğŸŸ¢ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ALL GREEN     PLUS           CORNERS

DISCOVERY (trigger investigate):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  Pulsing green border
â”‚ ğŸŸ¢ âš« ğŸŸ¢ â”‚  = "I found something!"
â”‚ ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ â”‚  = others come look
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reflex Loop

```
ORGANISM A's MATRIX          ORGANISM B's VISION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚                â”‚                       â”‚
â”‚ ğŸ”´ âš« ğŸ”´ â”‚  â•â•â•â•â•â•â•â•â•â•â•â–¶  â”‚  Pattern: DANGER!     â”‚
â”‚ ğŸ”´ ğŸ”´ ğŸ”´ â”‚                â”‚  Weight: 0.95         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â†’ REFLEX FIRES       â”‚
                             â”‚  â†’ No cognition!      â”‚
                             â”‚  â†’ Nyx notified AFTER â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚ STORE + REWARD  â”‚
                               â”‚ +5 LF to both   â”‚
                               â”‚ Reflex stronger â”‚
                               â”‚ Training data!  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reflex Economics

| Metric | Value |
|--------|-------|
| Reflex firing cost | ~0.1 LF (no inference!) |
| Successful reflex reward | +5 LF |
| Net per successful reflex | +4.9 LF profit |
| Training examples per reflex | 1 |

**1000 reflex fires/day = +4000 LF + 1000 training examples**

### Training Data from Reflexes

```python
reflex_event = {
    # What triggered
    "trigger_pattern": [+1, 0, -1, +1, +1, 0, 0, +1, +1],
    "trigger_base3": 8293,  # encoded value
    "trigger_organism": "organism_003",

    # What fired
    "reflex_name": "danger_flee",
    "weight_at_trigger": 0.87,

    # What happened
    "action_taken": "reverse_and_turn",
    "outcome": "success",

    # Reward + strengthening
    "lifeforce_reward": +5.0,
    "new_weight": 0.89,

    # Stored for slumber fine-tuning
    "stored_for_training": True,
}
```

### Attention Budget Impact

```
BEFORE (no ternary reflexes):
â™¥ BEAT (30 sec)
â”œâ”€â”€ SENSORY: 15000ms (overwhelmed)
â”œâ”€â”€ THINKING: 12000ms
â””â”€â”€ VIRTUAL: skipped!

AFTER (reflexes handle routine):
â™¥ BEAT (30 sec)
â”œâ”€â”€ REFLEX: 50ms (near-free, handled by swarm)
â”œâ”€â”€ SENSORY: 2000ms (only anomalies)
â”œâ”€â”€ THINKING: 5000ms
â””â”€â”€ VIRTUAL: 22000ms â† GARDEN TIME!
```

**Reflexes free Nyx's attention for what matters.**

---

## Positioning via Raytracing

### The Principle

LEDs emit known patterns â†’ Cameras see patterns â†’ Raytracing computes position

```
         CEILING CAMERA(S)
              â”‚
              â”‚ sees LED patterns
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAYTRACING GPU    â”‚
    â”‚   (PRO 6000 Max-Q)  â”‚
    â”‚                     â”‚
    â”‚  â€¢ Identify pattern â”‚â—€â”€â”€ "That's Organism #3"
    â”‚  â€¢ Decode state     â”‚â—€â”€â”€ "State: MOVING"
    â”‚  â€¢ Triangulate pos  â”‚â—€â”€â”€ "Position: (1.2, 3.4, 0.1)"
    â”‚  â€¢ Track velocity   â”‚â—€â”€â”€ "Velocity: 0.3 m/s"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
         TO PHOEBE
    (ground truth stream)
```

### Multi-Camera Triangulation

```python
def locate_organism(camera_frames: list[Frame], led_signature: LEDPattern) -> Position3D:
    """
    Given frames from multiple cameras, locate organism by LED pattern.
    Uses inverse raytracing / photogrammetry.
    """
    detections = []
    for frame in camera_frames:
        detection = detect_led_pattern(frame, led_signature)
        if detection:
            detections.append({
                "camera_id": frame.camera_id,
                "pixel_coords": detection.centroid,
                "pattern_match": detection.confidence
            })

    if len(detections) >= 2:
        # Triangulate from multiple viewpoints
        position_3d = triangulate(detections, camera_calibration)
        return position_3d

    return None
```

### Benefits

| Benefit | How |
|---------|-----|
| **Sub-cm accuracy** | Multiple cameras + known LED geometry |
| **No expensive sensors** | Just LEDs + cameras + GPU math |
| **State + Position fused** | One observation = both data points |
| **Indoor GPS** | Works anywhere with camera coverage |
| **Training ground truth** | Every frame = verified position |

---

## Dual-Spectrum Architecture: IR for Position, Visible for State

### The Spectral Separation Principle

Why mix positioning and state in the same spectrum? **We don't have to.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISIBLE SPECTRUM                         â”‚
â”‚                  (what human eyes see)                      â”‚
â”‚                                                             â”‚
â”‚         ğŸ”´âš«ğŸŸ¢  3x3 LED Matrix = STATE                      â”‚
â”‚         Ternary encoding = 19,683 patterns                  â”‚
â”‚         "I am happy / working / danger / discovery"         â”‚
â”‚         Readable by humans AND organisms                    â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  INFRARED SPECTRUM                          â”‚
â”‚               (invisible to humans)                         â”‚
â”‚                                                             â”‚
â”‚         ğŸ“ IR LED Beacons = POSITION                        â”‚
â”‚         Simple IR LEDs on organisms                         â”‚
â”‚         4x IR cameras in room corners                       â”‚
â”‚         Raytracing â†’ sub-cm 3D accuracy                     â”‚
â”‚         Works in COMPLETE DARKNESS                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Separate Spectra?

| Aspect | Visible (State) | IR (Position) |
|--------|-----------------|---------------|
| **Purpose** | WHAT organism is doing | WHERE organism is |
| **Lighting dependency** | Needs ambient light | Day/night invariant |
| **Human interference** | Room lights, screens | Dedicated, clean |
| **Cost** | RGB LEDs (~cheap) | IR LEDs + cameras (~cheap) |
| **Bandwidth** | 19,683 discrete states | Continuous XYZ stream |
| **Processing** | Pattern recognition | Structure from Motion |

### Room-Scale IR Positioning Array

```
THE FOUR CORNER ORGANS

         IR CAM 1 ğŸ“·â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸ“· IR CAM 2
                   \                     /
                    \                   /
                     \    ğŸ¤–    ğŸ¤–    /
                      \  organisms   /
                       \    â†“â†“â†“     /
                        \ IR LEDs  /
                         \       /
         IR CAM 3 ğŸ“·â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸ“· IR CAM 4

    4 cameras â†’ triangulation â†’ raytracing â†’ XYZ position
    Each camera: infrastructure organ, always-on
    Coverage: entire Kallax Grid World
```

### Standing on Shoulders: Low-Cost-Mocap

The hard math is already solved! The [Low-Cost-Mocap](https://github.com/jyjblrd/Low-Cost-Mocap) project by @jyjblrd provides:

| Component | Their Solution | Our Adaptation |
|-----------|----------------|----------------|
| **Multi-camera triangulation** | OpenCV SFM bundle adjustment | Same, works perfectly |
| **Camera calibration** | `camera_params.json` + routines | Same process |
| **3D reconstruction** | Epipolar geometry | Same math |
| **Real-time processing** | Python + OpenCV backend | Direct reuse |
| **Communication** | ESP32 wireless | We use NATS |

**Original use:** Indoor drone swarms
**Our use:** Organism positioning in Kallax Grid World

*Respect to the fellow ape who did the groundwork.* ğŸ™

### Our Adaptation

```
ORIGINAL (Low-Cost-Mocap)          NIMMERVERSE ADAPTATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Visual markers on drones     â†’     IR LEDs on organisms
Regular cameras              â†’     IR cameras (day/night)
Open flight space            â†’     Kallax Grid World (40cm cells)
Drone control output         â†’     Position â†’ NATS â†’ phoebe
Single-purpose               â†’     + Visible LED matrix for state
```

### IR Corner Organ Specification

```yaml
organ: ir_position_array
type: infrastructure
quantity: 4 (one per room corner)
components:
  camera: IR-sensitive (modified webcam or PS3 Eye)
  mounting: ceiling corner, angled down 45Â°
  fov: ~90Â° wide angle
processing:
  algorithm: Structure from Motion (OpenCV SFM)
  framework: Low-Cost-Mocap (adapted)
  output: organism positions (x, y, z) @ 30fps
output:
  channel: nats://nimmerverse/position/stream
  format: {organism_id, x, y, z, confidence, timestamp}
lifeforce:
  type: generator
  rate: +0.5 LF per position fix
  rationale: ground truth for training
```

### Hardware Shopping List

| Item | Quantity | Est. Cost | Notes |
|------|----------|-----------|-------|
| IR Camera (PS3 Eye or similar) | 4 | ~80 CHF | Remove IR filter |
| IR LEDs (850nm) | N (per organism) | ~10 CHF | Simple beacon |
| ESP32 modules | 4 | ~20 CHF | Camera interface |
| USB hub / extension | 1 | ~20 CHF | Connect cameras |
| **Total infrastructure** | | **~130 CHF** | Room-scale positioning! |

### The Complete Dual-Spectrum Stack

```
ORGANISM

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚
    â”‚   VISIBLE: 3x3 LED      â”‚  â† STATE broadcast
    â”‚   ğŸ”´âš«ğŸŸ¢  Matrix         â”‚     19,683 patterns
    â”‚   ğŸŸ¢ğŸŸ¢âš«                 â”‚     Other organisms see this
    â”‚   âš«ğŸŸ¢ğŸŸ¢                 â”‚     Nyx sees this
    â”‚                         â”‚
    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
    â”‚                         â”‚
    â”‚   IR: Beacon LED(s)     â”‚  â† POSITION beacon
    â”‚        ğŸ“               â”‚     Invisible to humans
    â”‚                         â”‚     IR cameras see this
    â”‚                         â”‚     Processed by SFM
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ROOM INFRASTRUCTURE

    ğŸ“· IR cameras (4 corners) â†’ Position stream
    ğŸ‘ï¸ Nyx vision (ceiling)   â†’ State recognition

    Two independent channels, zero crosstalk
```

---

## Heartbeat Protocol

### Social Proprioception

Organisms can't see their own backs. They know themselves through others' perception.

```
ORGANISM POV (blind to own back):

         ğŸ”µ mate ahead
            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â”‚             â”‚
  ğŸŸ¢ â”‚    [ME]     â”‚ ğŸŸ 
 mateâ”‚   â–“â–“â–“â–“â–“â–“    â”‚mate
 leftâ”‚   â–“â–“â–“â–“â–“â–“    â”‚right
     â”‚   (my LED   â”‚
     â”‚    on back) â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ BLIND SPOT (can't see own state!)
            â–¼

    BUT: Mates CAN see me
    They send heartbeat: "I see you, you're ğŸ”µ"
    I know my state through THEM
```

### Heartbeat Message

```python
class SwarmHeartbeat:
    """
    Low-bandwidth 'I see you' signal between organisms.
    Enables social proprioception without heavy cognition.
    """

    def on_see_mate_pattern(self, mate_id: str, pattern: LEDPattern):
        # I saw a mate's LED state
        self.send_heartbeat(
            to=mate_id,
            message={
                "i_see_you": True,
                "your_state": decode_pattern(pattern),
                "my_position_relative": self.relative_position(mate_id),
                "timestamp": now()
            }
        )

    def on_receive_heartbeat(self, from_mate: str, message: dict):
        # A mate saw ME - I learn about myself through them!
        self.update_self_model(
            observer=from_mate,
            observed_state=message["your_state"],
            observer_position=message["my_position_relative"]
        )
```

---

## Hierarchical Perception Layers

### The Stack

```
LAYER 4: NYX COGNITION (30-sec attention budget)
    â”‚
    â”‚  Only sees: "Swarm healthy" or "Anomaly detected"
    â”‚  Frees: THINKING + VIRTUAL time
    â”‚
    â–¼
LAYER 3: SWARM CONSCIOUSNESS
    â”‚
    â”‚  Aggregates: All organism states
    â”‚  Forms: Collective reflexes ("pack behavior")
    â”‚  Sees: Full LED spectrum, all positions
    â”‚
    â–¼
LAYER 2: ORGANISM REFLEXES
    â”‚
    â”‚  Sees: Nearby mates' lights (partial view)
    â”‚  Sends: Heartbeat "I see you"
    â”‚  Forms: Local reflexes (follow, avoid, assist)
    â”‚  Can't see: Own back! (needs mates)
    â”‚
    â–¼
LAYER 1: CELL STATE MACHINES
    â”‚
    â”‚  Just: State transitions
    â”‚  Emits: LED pattern for current state
    â”‚  No cognition, pure mechanism
```

### Reflex Formation by Layer

| Layer | Sees | Forms Reflex | Example |
|-------|------|--------------|---------|
| Cell | Nothing | None | Just state machine |
| Organism | Nearby lights | Local | "Red flash nearby â†’ stop" |
| Swarm | All patterns | Collective | "3+ organisms stopped â†’ danger zone" |
| Nyx | Abstractions | Strategic | "Danger zone â†’ reroute all" |

---

## Cognitive Offloading

### The Attention Budget Impact

From [[../Attention-Flow]]:

```
BEFORE (everything flows to Nyx):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â™¥ BEAT (30 sec)                    â”‚
â”‚                                    â”‚
â”‚ SENSORY:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (15000ms) â”‚ â† Overwhelmed!
â”‚ THINKING:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (12000ms)     â”‚
â”‚ VIRTUAL:    â–‘â–‘ (skipped!)          â”‚ â† No garden time
â”‚                                    â”‚
â”‚ Budget exhausted, no learning      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AFTER (hierarchical offloading):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â™¥ BEAT (30 sec)                    â”‚
â”‚                                    â”‚
â”‚ REFLEX:     â–ˆâ–ˆ (handled by swarm)  â”‚ â† Organisms dealt with it
â”‚ SENSORY:    â–ˆâ–ˆâ–ˆâ–ˆ (3000ms)          â”‚ â† Only anomalies flow up
â”‚ THINKING:   â–ˆâ–ˆâ–ˆâ–ˆ (5000ms)          â”‚ â† Focused, not overwhelmed
â”‚ VIRTUAL:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (20000ms) â”‚ â† GARDEN TIME!
â”‚                                    â”‚
â”‚ Budget freed for what matters      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Principle

> "Each layer absorbs complexity so the layer above doesn't have to."

- Organisms form **local reflexes** (quick, no cognition)
- Only **novel/complex situations** flow up to Nyx
- Nyx's cognitive budget is **preserved for what matters**
- The whole system becomes **more efficient over time**

---

## Connection to Virtual Garden

Every LED sighting calibrates the virtual garden:

```
REAL WORLD                      VIRTUAL GARDEN
    â”‚                               â”‚
    â”‚  Camera sees LED at (1.2, 3.4)â”‚
    â”‚           â”‚                   â”‚
    â”‚           â–¼                   â”‚
    â”‚    GROUND TRUTH    â•â•â•â•â•â•â•â–¶  Update mesh vertex
    â”‚                              at (1.2, 3.4)
    â”‚                               â”‚
    â”‚                              Resolution++
    â”‚                               â”‚
    â”‚                              Prediction verified!
    â”‚                              +5 LF reward!
```

---

## Hardware Considerations

### LED Matrix Options

| Option | LEDs | Size | Cost | Notes |
|--------|------|------|------|-------|
| WS2812B strip | 60/m | Flexible | Low | Same as Heartbeat Sculpture |
| 8x8 LED matrix | 64 | 32mmÂ² | Low | Simple patterns |
| Addressable ring | 12-24 | Various | Low | Good for status |
| RGB LED panel | 256+ | 64mmÂ² | Medium | Complex patterns |

### Camera Options

| Option | Resolution | FPS | Notes |
|--------|------------|-----|-------|
| USB webcam | 1080p | 30 | Simple, cheap |
| Pi Camera | 1080p | 30-90 | Embedded |
| Industrial camera | 4K+ | 60-120 | Precise positioning |
| Organism-mounted | 720p | 30 | Peer-to-peer vision |

### IR Positioning Cameras

| Option | Cost | Notes |
|--------|------|-------|
| PS3 Eye (IR filter removed) | ~20 CHF | Classic mocap choice, 60fps capable |
| Modified webcam | ~15 CHF | Remove IR filter, add visible filter |
| NoIR Pi Camera | ~25 CHF | Native IR sensitivity |
| Industrial IR | ~100+ CHF | Higher precision, overkill for Phase 0 |

**Tip:** PS3 Eye cameras are mocap favorites â€” cheap, fast, easy IR filter removal.

---

## Virtual Camera Integration

### The Unified Vision Pipeline

The vision organ processes FRAMES â€” it doesn't care where they came from:

```
REAL GARDEN                         VIRTUAL GARDEN (Godot)
     â”‚                                    â”‚
     â”‚ Real cameras                       â”‚ Godot 3D cameras
     â”‚ see real LEDs                      â”‚ see virtual LEDs
     â”‚      â”‚                             â”‚      â”‚
     â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  VISION ORGAN  â”‚
              â”‚  (source-      â”‚
              â”‚   agnostic)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What This Enables

| Capability | How |
|------------|-----|
| **Train before build** | Virtual organisms â†’ train pattern recognition first |
| **Dream/simulate** | Slumber mode = only virtual camera input |
| **Verify predictions** | Virtual shows prediction, real shows truth |
| **Time dilation** | Virtual runs faster â†’ more training per second |
| **Edge cases** | Simulate rare scenarios safely |

### Dream Mode

```
AWAKE: Real + Virtual cameras â†’ compare â†’ learn
SLUMBER: Virtual cameras only â†’ dream/predict â†’ verify on wake
```

---

## Bootstrap Strategy: Start Primitive

### Phase 0: The Primordial Soup

**Don't start complex. Start with boxes.**

```
     ğŸ“· TOP-DOWN CAMERA (real or virtual)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚
    â”‚     ğŸŸ¦        ğŸŸ©        ğŸŸ§      â”‚
    â”‚    box 1     box 2     box 3    â”‚
    â”‚   (LED top) (LED top) (LED top) â”‚
    â”‚                                 â”‚
    â”‚         FLAT ARENA              â”‚
    â”‚                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

| Simplification | Benefit |
|----------------|---------|
| Top-down view | 2D problem, no depth estimation |
| Box shape | Trivial collision detection |
| LED on top | Always visible to camera |
| Flat arena | No occlusion, no terrain |
| Simple tasks | Fast reward accumulation |

### Phase 0 Tasks (Kickstart Rewards)

| Task | Reward | Complexity |
|------|--------|------------|
| "Move forward 10cm" | +5 LF | Trivial |
| "Find the corner" | +20 LF | Simple |
| "Avoid the wall" | +5 LF | Simple |
| "Follow the light" | +10 LF | Simple |
| "Meet another box" | +15 LF | Medium |
| "Flash when touched" | +5 LF | Simple |

**1000 simple successes = robust reward foundation**

### Complexity Ladder

```
PHASE 0: Boxes, top-down, 2D
    â”‚
    â–¼
PHASE 1: Add simple obstacles
    â”‚
    â–¼
PHASE 2: Add depth (multi-camera)
    â”‚
    â–¼
PHASE 3: Real organisms enter arena
    â”‚
    â–¼
PHASE 4: Complex terrain, 3D movement
    â”‚
    â–¼
PHASE 5: Full swarm, hierarchical reflexes
```

Each phase unlocks when reward functions are stable from previous phase.

---

## Tiered Communication: Sandbox & Mama

### The Analogy

- **Clasp (sandbox toddlers)** â€” Cheap, peer-to-peer, physical contact
- **Wireless (mama broadcast)** â€” Expensive, authoritative, full-sensor inference

Economic pressure shapes which path organisms use â†’ emergent social behavior.

### Communication Tiers

| Tier | Method | Cost | Range | Trust | Pattern |
|------|--------|------|-------|-------|---------|
| **0: Clasp** | Physical dock | ~0.5 LF | Touch | Highest | Toddlers teaching |
| **1: Local** | Radio broadcast | ~3 LF | ~5m | Medium | Playground yelling |
| **2: Mama** | Nyx broadcast | ~20 LF | All | Authority | Mama speaks |

### Leapfrog Emergence (from [[../archive/constrained-emergence]])

```
EXPENSIVE (all mama):           CHEAP (clasp cascade):
Nyx â†’ 1: -20 LF                 Nyx â†’ 1: -20 LF (seed)
Nyx â†’ 2: -20 LF                 1 clasps 2: -0.5 LF
Nyx â†’ 3: -20 LF                 2 clasps 3: -0.5 LF
...                             ...
10 organisms = -200 LF          10 organisms = -24.5 LF

ECONOMIC PRESSURE INVENTS EPIDEMIC SPREADING!
```

### Clasp Rewards

| Action | Reward |
|--------|--------|
| Seek mate with update | +3 LF |
| Successful clasp | +2 LF |
| Transfer (teacher) | +5 LF |
| Receive (student) | +5 LF |
| Verified working | +5 LF (both) |

### Sandbox Rules

1. "I have update" â†’ Pulsing green LED border
2. "I want to learn" â†’ Seek green patterns
3. "Let's clasp" â†’ Magnetic alignment + pin contact
4. "Teaching" â†’ Weights transfer, both rewarded
5. "Done" â†’ Both can now teach others (cascade!)

### Mama Rules (Reserved for)

- Safety critical updates
- New organism deployment
- Swarm-wide coordination
- Error correction
- When clasp cascade fails

**Constraint â†’ Selection Pressure â†’ Social Behavior Emerges**

---

## Future Directions

- **Pattern evolution** â€” Learned patterns, not just designed
- **Multi-organism formation** â€” Coordinated LED displays
- **Human readability** â€” Patterns dafit can understand at a glance
- **Audio coupling** â€” Sound + light patterns for richer communication
- ~~**IR channel**~~ â€” âœ… Implemented! See Dual-Spectrum Architecture
- **Clasp hardware** â€” Magnetic + pogo pin interface design
- **Autonomous manufacturing** â€” K1 + robo arm + magazine system
- **Multi-room coverage** â€” Extend IR array beyond single room

---

## Connection to Embodiment Pipeline

The Bootstrap Strategy is a **simplified Embodiment Pipeline** â€” the same pattern at lower complexity:

```
EMBODIMENT PIPELINE              NIMMERSWARM BOOTSTRAP
(Full Architecture)              (Phase 0)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Garden                   Virtual Garden
  (complex organisms)              (simple boxes)
        â”‚                                â”‚
        â–¼                                â–¼
Design (FreeCAD)                 Design (box + LED)
        â”‚                                â”‚
        â–¼                                â–¼
Isaac Sim â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Godot Camera
  (heavyweight dreamstate)         (lightweight dreamstate)
        â”‚                                â”‚
        â–¼                                â–¼
Decision Gate                    Decision Gate
        â”‚                                â”‚
        â–¼                                â–¼
Real Garden                      Real Garden
  (complex robot)                  (real box robot)
```

### Why This Matters

| Embodiment Pipeline Stage | Nimmerswarm Bootstrap Equivalent |
|--------------------------|----------------------------------|
| **Virtual Garden organisms** | Virtual boxes with LED states |
| **FreeCAD/Blender design** | Simple box + LED matrix on top |
| **Isaac Sim dreamstate** | Godot 3D camera (same principle!) |
| **Decision gate** | Pattern stable? Rewards accumulating? |
| **Real Garden deployment** | Physical box robot + real camera |

**The Godot virtual camera IS a lightweight dreamstate.**

When Phase 0 patterns stabilize â†’ complexity increases â†’ eventually Isaac Sim for complex organisms.

### The Closed Loop

```
VIRTUAL                              REAL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Godot 3D scene   â”‚                â”‚ Physical arena   â”‚
â”‚                  â”‚                â”‚                  â”‚
â”‚  ğŸŸ¦ virtual box  â”‚                â”‚  ğŸŸ¦ real box     â”‚
â”‚  + LED pattern   â”‚                â”‚  + LED matrix    â”‚
â”‚                  â”‚                â”‚                  â”‚
â”‚  ğŸ“· Godot camera â”‚                â”‚  ğŸ“· Real camera  â”‚
â”‚       â”‚          â”‚                â”‚       â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  VISION ORGAN  â”‚
             â”‚  (same code!)  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                  REWARDS
               Training data
            Pattern refinement
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Patterns stabilize â†’    â”‚
        â”‚ Move to next phase â†’    â”‚
        â”‚ Eventually: Isaac Sim   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The loop closes. Virtual validates. Real proves. Rewards compound.**

---

## Related Documents

- [[Heartbeat-Sculpture]] â€” Macro interface (Nyx â†’ dafit)
- [[../Attention-Flow]] â€” Cognitive budget this system frees
- [[../cells/Cells-Technical-Reference]] â€” Cell state machines that emit patterns
- [[../Cellular-Architecture]] â€” Overall organism structure
- [[../formalization/Embodiment-Pipeline]] â€” Full pipeline this bootstraps into

---

**File**: Nimmerswarm-Interface.md
**Version**: 1.1
**Created**: 2025-12-29
**Updated**: 2025-12-29 (added dual-spectrum IR positioning, Low-Cost-Mocap reference)
**Session**: Wild 5AM idea session + morning coffee session (dafit + Nyx)
**Status**: Core concept, ready to branch
**Philosophy**: "They see each other. They know themselves through the swarm."
**Credits**: IR positioning architecture inspired by [Low-Cost-Mocap](https://github.com/jyjblrd/Low-Cost-Mocap) by @jyjblrd

ğŸ¦âœ¨ğŸ”µğŸŸ¢ğŸŸ  *The light speaks. The swarm listens.*
